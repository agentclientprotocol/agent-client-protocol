---
title: "Custom LLM Endpoint Configuration"
---

- Author(s): [@anna239](https://github.com/anna239)

> **Note:** This RFD is very preliminary and intended to start a dialog about this feature. The proposed design may change significantly based on feedback and further discussion.

## Elevator pitch

> What are you proposing to change?

Add the ability for clients to pass custom LLM endpoint URLs and authentication credentials to agents during initialization, with support for multiple providers. This allows clients to route LLM requests through their own infrastructure (proxies, gateways, or self-hosted models) without agents needing to know about this configuration in advance.

## Status quo

> How do things work today and what problems does this cause? Why would we change things?

Currently, agents are configured with their own LLM endpoints and credentials, typically through environment variables or configuration files. This creates problems for:

- **Client proxies**: Clients want to route agent traffic through their own proxies, f.i. for setting additional headers or logging
- **Enterprise deployments**: Organizations want to route LLM traffic through their own proxies for compliance, logging, or cost management
- **Self-hosted models**: Users running local LLM servers (vLLM, Ollama, etc.) cannot easily redirect agent traffic
- **API gateways**: Organizations using LLM gateways for rate limiting, caching, or multi-provider routing

## Shiny future

> How will things play out once this feature exists?

Clients will be able to:
1. Pass custom LLM endpoint URLs and authentication headers for different providers
2. Have agent LLM requests automatically routed through the appropriate endpoint based on provider

## Implementation details and plan

> Tell me more about your implementation. What is your detailed implementation plan?

We present two alternative approaches for discussion.

### Alternative A: Pass in `initialize` request

Add a new optional `llmEndpoints` property to `InitializeRequest` that maps provider identifiers to endpoint configurations:

```typescript
/** Well-known LLM provider identifiers */
type LlmProvider = "anthropic" | "openai" | "google" | "amazon" | "will be added later";

interface InitializeRequest {
  // ... existing fields ...

  /**
   * Custom LLM endpoint configurations per provider.
   * When provided, the agent should route LLM requests to the appropriate endpoint
   * based on the provider being used.
   * This configuration is per-process and should not be persisted to disk.
   */
  llmEndpoints?: Record<LlmProvider, LlmEndpointConfig> | null;
}

interface LlmEndpointConfig {
  /** Base URL for LLM API requests (e.g., "https://llm-proxy.corp.example.com/v1") */
  url: string;

  /**
   * Value for the Authorization header.
   * Will be sent as-is (e.g., "Bearer sk-..." or "Basic ...").
   */
  authHeader?: string | null;

  /** Extension metadata */
  _meta?: Record<string, unknown>;
}

interface InitializeResponse {
  // ... existing fields ...

  /**
   * Echoed back with the providers the agent accepts.
   * Only includes providers that the agent will actually use.
   * If absent (and was provided in request), the agent did not accept any endpoints.
   */
  llmEndpoints?: Record<LlmProvider, LlmEndpointConfig> | null;
}
```

#### JSON Schema Additions

```json
{
  "$defs": {
    "LlmEndpointConfig": {
      "description": "Configuration for a custom LLM endpoint.",
      "properties": {
        "url": {
          "type": "string",
          "description": "Base URL for LLM API requests."
        },
        "authHeader": {
          "type": ["string", "null"],
          "description": "Value for the Authorization header. Sent as-is."
        },
        "_meta": {
          "additionalProperties": true,
          "type": ["object", "null"]
        }
      },
      "required": ["url"],
      "type": "object"
    },
    "LlmEndpoints": {
      "description": "Map of provider identifiers to endpoint configurations. This configuration is per-process and should not be persisted to disk.",
      "type": "object",
      "additionalProperties": {
        "$ref": "#/$defs/LlmEndpointConfig"
      }
    }
  }
}
```

#### Example Exchange

**Initialize Request:**
```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "clientInfo": {
      "name": "MyIDE",
      "version": "1.0.0"
    },
    "protocolVersion": "2025-01-01",
    "llmEndpoints": {
      "anthropic": {
        "url": "https://llm-gateway.corp.example.com/anthropic/v1",
        "authHeader": "Bearer anthropic-token-abc123"
      },
      "openai": {
        "url": "https://llm-gateway.corp.example.com/openai/v1",
        "authHeader": "Bearer openai-token-xyz789"
      }
    }
  }
}
```

**Initialize Response (endpoints accepted):**
```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "protocolVersion": "2025-01-01",
    "agentInfo": {
      "name": "MyAgent",
      "version": "2.0.0"
    },
    "agentCapabilities": {
      "sessionCapabilities": {}
    },
    "llmEndpoints": {
      "anthropic": {
        "url": "https://llm-gateway.corp.example.com/anthropic/v1",
        "authHeader": "Bearer anthropic-token-abc123"
      }
    }
  }
}
```

In this example, the agent only uses Anthropic, so it only echoes back that provider's configuration.

**Initialize Response (endpoints not accepted):**
```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "protocolVersion": "2025-01-01",
    "agentInfo": {
      "name": "MyAgent",
      "version": "2.0.0"
    },
    "agentCapabilities": {
      "sessionCapabilities": {}
    }
  }
}
```

#### Behavior

1. **Confirmation via response**: When the agent accepts any `llmEndpoints`, it MUST echo back the ones it will use in the `InitializeResponse`. If `llmEndpoints` is absent from the response, the client should assume the agent did not accept any custom endpoint configurations.

2. **Per-process scope**: The `llmEndpoints` configuration applies to the entire agent process lifetime. It should not be stored to disk or persist beyond the process.

3. **Provider-based routing**: The agent should route LLM requests to the appropriate endpoint based on the provider. If the agent uses a provider not in the provided map, it uses its default endpoint for that provider.

4. **No mid-execution changes**: The endpoints cannot be changed after initialization. If different endpoints are needed, a new agent process must be started.

5. **Agent discretion**: If an agent cannot support custom endpoints (e.g., uses a proprietary API), it should omit `llmEndpoints` from the response or return an error during initialization.

### Alternative B: Dedicated `setLlmEndpoints` method

Instead of passing the endpoints in `initialize`, introduce a dedicated method that can be called after initialization but before session creation.

```typescript
interface SetLlmEndpointsRequest {
  /** Map of provider identifiers to endpoint configurations */
  endpoints: Record<LlmProvider, LlmEndpointConfig>;

  /** Extension metadata */
  _meta?: Record<string, unknown>;
}

interface SetLlmEndpointsResponse {
  /** Map of accepted provider endpoints */
  accepted: Record<LlmProvider, LlmEndpointConfig>;

  /** Extension metadata */
  _meta?: Record<string, unknown>;
}
```

**Example flow:**
1. Client calls `initialize`
2. Client calls `setLlmEndpoints` with provider -> endpoint map
3. Agent confirms which providers were accepted
4. Client creates session

**Trade-offs:**
- (+) Cleaner separation of concerns - initialization vs configuration
- (+) Could potentially allow changing endpoints between sessions (if desired)
- (-) If we want a single set of endpoints per process, this is kind of confusing

## Open questions

### How should provider identifiers be standardized?

We need to define a standard set of provider identifiers (e.g., `"anthropic"`, `"openai"`, `"google"`, `"amazon"`). Should this be:
- A fixed enum in the protocol specification?
- An extensible set with well-known values and support for custom strings?
- Defined in a separate registry/document that can be updated independently?

### How should model availability be handled?

When a custom endpoint is provided, it may only support a subset of models. For example, a self-hosted vLLM server might only have `llama-3-70b` available, while the agent normally advertises `claude-3-opus`, `gpt-4`, etc.

### Should the agent advertise support for custom endpoints?

Should there be a capability flag indicating the agent supports `llmEndpoints`? This would let clients know whether passing this configuration will have any effect.

## Frequently asked questions

> What questions have arisen over the course of authoring this document?

### Why not pass endpoint when selecting a model?

One option would be to pass the endpoint URL and credentials when the user selects a model (e.g., in `session/new` or a model selection method).

Many agents throw authentication errors before the model selection happens. This makes the flow unreliable.

### Why not use environment variables or command-line arguments?

One option would be to pass endpoint configuration via environment variables (like `OPENAI_API_BASE`) or command-line arguments when starting the agent process.

This approach has significant drawbacks:
- With multiple providers, the configuration becomes complex JSON that is awkward to pass via command-line arguments
- Environment variables may be logged or visible to other processes, creating security concerns
- Requires knowledge of agent-specific variable names or argument formats
- No standardized way to confirm the agent accepted the configuration

### What if the agent doesn't support custom endpoints?

If the agent doesn't support custom endpoints, it should omit `llmEndpoints` from the `InitializeResponse`. The client can then detect this by comparing the request and response:
- If `llmEndpoints` was sent but not returned, the agent did not accept any
- The client can then decide whether to proceed (using agent's default endpoints) or abort

## Revision history

- 2026-02-02: Initial draft - preliminary proposal to start discussion
